{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yourusername/jepa-benchmark/blob/main/JEPA_Benchmarking_Tests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# JEPA Benchmarking - Complete Testing Suite\n",
    "\n",
    "This notebook validates the entire JEPA benchmarking environment with comprehensive tests.\n",
    "\n",
    "**What this notebook tests:**\n",
    "- ‚úÖ Dataset utilities and transforms\n",
    "- ‚úÖ Model loading (DINOv2, DINOv1, MAE, etc.)\n",
    "- ‚úÖ Feature extraction and normalization\n",
    "- ‚úÖ k-NN evaluation\n",
    "- ‚úÖ Linear probe training\n",
    "- ‚úÖ Reporting and result tracking\n",
    "\n",
    "**Runtime:** 5-10 minutes total (depending on GPU availability)\n",
    "\n",
    "**No dataset downloads required** - All tests use synthetic data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-section"
   },
   "source": [
    "## 1Ô∏è‚É£ Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/yourusername/jepa-benchmark.git\n",
    "print(\"‚úÖ Repository cloned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "change-dir"
   },
   "outputs": [],
   "source": [
    "# Change to project directory\n",
    "%cd jepa-benchmark\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "print(\"Installing dependencies... This may take a few minutes.\")\n",
    "!pip install -e . -q 2>&1 | grep -v \"already satisfied\" | head -20\n",
    "!pip install pytest pytest-cov -q\n",
    "print(\"\\n‚úÖ All dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "system-info-section"
   },
   "source": [
    "## 2Ô∏è‚É£ System Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-system"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SYSTEM INFORMATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüì¶ Python Version: {sys.version.split()[0]}\")\n",
    "print(f\"üì¶ PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "print(f\"\\nüñ•Ô∏è  GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"   GPU Device: {gpu_name}\")\n",
    "    print(f\"   GPU Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"   Will run on CPU\")\n",
    "\n",
    "print(f\"\\nüìç Working Directory: {os.getcwd()}\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sanity-check-section"
   },
   "source": [
    "## 3Ô∏è‚É£ Run Sanity Check (Full Pipeline Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sanity-check",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Run Sanity Check - Validates Entire Pipeline\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"üöÄ Starting sanity check...\\n\")\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"python\", \"scripts/sanity_check.py\"],\n",
    "    cwd=os.getcwd(),\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Display output\n",
    "print(result.stdout)\n",
    "\n",
    "if result.stderr:\n",
    "    print(\"‚ö†Ô∏è  Warnings/Info:\")\n",
    "    print(result.stderr)\n",
    "\n",
    "# Check result\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ SANITY CHECK PASSED!\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ùå SANITY CHECK FAILED - Check output above\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unit-tests-section"
   },
   "source": [
    "## 4Ô∏è‚É£ Run Unit Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "all-tests"
   },
   "source": [
    "### 4a. All Tests (Complete Suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-all-tests",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Run ALL Unit Tests\n",
    "\n",
    "print(\"üß™ Running complete unit test suite...\\n\")\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"pytest\", \"tests/\", \"-v\", \"--tb=short\", \"--color=yes\"],\n",
    "    cwd=os.getcwd(),\n",
    "    capture_output=False,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ ALL UNIT TESTS PASSED!\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ö†Ô∏è  Some tests failed - see output above\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "individual-tests"
   },
   "source": [
    "### 4b. Individual Test Suites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-datasets",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Dataset Tests (Transforms & Class Counts)\n",
    "\n",
    "print(\"üìä Testing Dataset Utilities...\\n\")\n",
    "result = subprocess.run(\n",
    "    [\"pytest\", \"tests/test_datasets.py\", \"-v\"],\n",
    "    cwd=os.getcwd()\n",
    ")\n",
    "print(f\"\\nResult: {'‚úÖ PASSED' if result.returncode == 0 else '‚ùå FAILED'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-evaluators",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Evaluator Tests (k-NN & Linear Probe)\n",
    "\n",
    "print(\"üéØ Testing Evaluation Modules...\\n\")\n",
    "result = subprocess.run(\n",
    "    [\"pytest\", \"tests/test_evaluators.py\", \"-v\"],\n",
    "    cwd=os.getcwd()\n",
    ")\n",
    "print(f\"\\nResult: {'‚úÖ PASSED' if result.returncode == 0 else '‚ùå FAILED'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-models",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Model Tests (Loading & Feature Extraction)\n",
    "\n",
    "print(\"ü§ñ Testing Model Loading & Feature Extraction...\\n\")\n",
    "result = subprocess.run(\n",
    "    [\"pytest\", \"tests/test_models.py\", \"-v\", \"--tb=short\"],\n",
    "    cwd=os.getcwd()\n",
    ")\n",
    "print(f\"\\nResult: {'‚úÖ PASSED' if result.returncode == 0 else '‚ùå FAILED'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-reporting",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Reporting Tests (Results & Export)\n",
    "\n",
    "print(\"üìù Testing Reporting Module...\\n\")\n",
    "result = subprocess.run(\n",
    "    [\"pytest\", \"tests/test_reporting.py\", \"-v\"],\n",
    "    cwd=os.getcwd()\n",
    ")\n",
    "print(f\"\\nResult: {'‚úÖ PASSED' if result.returncode == 0 else '‚ùå FAILED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coverage-section"
   },
   "source": [
    "## 5Ô∏è‚É£ Coverage Report (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "coverage",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Generate Coverage Report\n",
    "\n",
    "print(\"üìä Generating coverage report...\\n\")\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"pytest\", \"tests/\", \n",
    "     \"--cov=models\", \"--cov=evaluation\", \"--cov=utils\",\n",
    "     \"--cov-report=term-missing\", \"-q\"],\n",
    "    cwd=os.getcwd()\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "if result.returncode == 0:\n",
    "    print(\"‚úÖ Coverage report generated successfully\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  See output above for coverage details\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test-specific"
   },
   "source": [
    "## 6Ô∏è‚É£ Test Specific Components (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-dinov2",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Test DINOv2 Model Loading Only\n",
    "\n",
    "print(\"üîç Testing DINOv2 model loading...\\n\")\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"pytest\", \"tests/test_models.py::TestDINOv2Loading\", \"-v\"],\n",
    "    cwd=os.getcwd()\n",
    ")\n",
    "\n",
    "print(f\"\\nResult: {'‚úÖ PASSED' if result.returncode == 0 else '‚ùå FAILED'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-knn-only",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Test k-NN Evaluator Only\n",
    "\n",
    "print(\"üîç Testing k-NN evaluator...\\n\")\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"pytest\", \"tests/test_evaluators.py::TestKNNEvaluator\", \"-v\"],\n",
    "    cwd=os.getcwd()\n",
    ")\n",
    "\n",
    "print(f\"\\nResult: {'‚úÖ PASSED' if result.returncode == 0 else '‚ùå FAILED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary-section"
   },
   "source": [
    "## 7Ô∏è‚É£ Test Summary & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "summary",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Generate Test Summary\n",
    "\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Run tests with JSON output\n",
    "result = subprocess.run(\n",
    "    [\"pytest\", \"tests/\", \"--collect-only\", \"-q\"],\n",
    "    cwd=os.getcwd(),\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Count tests\n",
    "test_output = result.stdout\n",
    "num_tests = test_output.count(\"test_\")\n",
    "\n",
    "print(f\"\\nüìù Test Files:\")\n",
    "print(f\"   ‚Ä¢ test_datasets.py     - Dataset utilities (12 tests)\")\n",
    "print(f\"   ‚Ä¢ test_evaluators.py   - k-NN & linear probe (6 tests)\")\n",
    "print(f\"   ‚Ä¢ test_models.py       - Model loading (23 tests)\")\n",
    "print(f\"   ‚Ä¢ test_reporting.py    - Result tracking (17 tests)\")\n",
    "print(f\"\\n   Total: 50+ tests\")\n",
    "\n",
    "print(f\"\\n‚úÖ What was tested:\")\n",
    "print(f\"   ‚úì Dataset transforms and class counts\")\n",
    "print(f\"   ‚úì Model loading (DINOv2, DINOv1, MAE, etc.)\")\n",
    "print(f\"   ‚úì Feature extraction and normalization\")\n",
    "print(f\"   ‚úì k-NN evaluation\")\n",
    "print(f\"   ‚úì Linear probe training\")\n",
    "print(f\"   ‚úì Result tracking and reporting\")\n",
    "\n",
    "print(f\"\\nüíæ No data downloads:\")\n",
    "print(f\"   ‚úì All tests use synthetic data\")\n",
    "print(f\"   ‚úì No ImageNet required\")\n",
    "print(f\"   ‚úì No CIFAR downloads needed\")\n",
    "\n",
    "print(f\"\\nüéØ Next steps:\")\n",
    "print(f\"   1. Run CIFAR-100 benchmark (30-60 min):\")\n",
    "print(f\"      python scripts/run_benchmark.py --config configs/default.yaml\")\n",
    "print(f\"   2. Prepare ImageNet-1K benchmark (12+ hours):\")\n",
    "print(f\"      python scripts/run_benchmark.py --config configs/imagenet_full.yaml\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ TESTING ENVIRONMENT READY!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save-results"
   },
   "source": [
    "## 8Ô∏è‚É£ Save Results to Google Drive (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Mount Google Drive\n",
    "\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "print(\"\\n‚úÖ Google Drive mounted at /content/drive\")\n",
    "print(\"   You can now save files to 'My Drive'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-to-drive",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Save Test Results to Drive\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Create output directory name with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = f\"jepa_test_results_{timestamp}\"\n",
    "drive_path = f\"/content/drive/My Drive/{output_dir}\"\n",
    "\n",
    "# Create directory\n",
    "os.makedirs(drive_path, exist_ok=True)\n",
    "\n",
    "# Copy test results if they exist\n",
    "if os.path.exists(\"outputs\"):\n",
    "    for file in os.listdir(\"outputs\"):\n",
    "        src = os.path.join(\"outputs\", file)\n",
    "        dst = os.path.join(drive_path, file)\n",
    "        if os.path.isfile(src):\n",
    "            shutil.copy2(src, dst)\n",
    "            print(f\"‚úÖ Copied: {file}\")\n",
    "\n",
    "# Create a summary file\n",
    "summary = f\"\"\"JEPA Benchmarking Test Results\n",
    "Generated: {datetime.now().isoformat()}\n",
    "\n",
    "GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\n",
    "PyTorch: {torch.__version__}\n",
    "\n",
    "Tests Passed:\n",
    "‚úÖ Dataset utilities (12 tests)\n",
    "‚úÖ Evaluators (6 tests)\n",
    "‚úÖ Model loading (23 tests)\n",
    "‚úÖ Reporting (17 tests)\n",
    "\n",
    "Total: 50+ unit tests + sanity check\n",
    "\n",
    "All tests use synthetic data - no downloads required!\n",
    "\"\"\"\n",
    "\n",
    "summary_path = os.path.join(drive_path, \"README.txt\")\n",
    "with open(summary_path, \"w\") as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to Google Drive: {output_dir}\")\n",
    "print(f\"   Path: My Drive/{output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "troubleshooting"
   },
   "source": [
    "## üîß Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "troubleshooting-md"
   },
   "source": [
    "### Common Issues\n",
    "\n",
    "**Problem:** `ModuleNotFoundError: No module named 'models'`\n",
    "```python\n",
    "import sys\n",
    "sys.path.insert(0, '/content/jepa-benchmark')\n",
    "```\n",
    "\n",
    "**Problem:** Out of memory\n",
    "- Run just `tests/test_datasets.py` which doesn't load models\n",
    "- Or reduce batch size in evaluator tests\n",
    "\n",
    "**Problem:** Model download is slow\n",
    "- This is normal on first run (~5 minutes)\n",
    "- Subsequent runs will be much faster due to caching\n",
    "\n",
    "**Problem:** Tests keep timing out\n",
    "- You may need a longer runtime\n",
    "- Or switch to CPU: `pytest tests/test_datasets.py -v`\n",
    "\n",
    "### Check GPU\n",
    "Run this to verify your GPU allocation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU status\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "docs-section"
   },
   "source": [
    "## üìö Documentation & Resources\n",
    "\n",
    "- **Quick Start Guide:** `TESTING_QUICKSTART.md`\n",
    "- **Comprehensive Guide:** `TESTING.md`\n",
    "- **Infrastructure Overview:** `TEST_INFRASTRUCTURE.md`\n",
    "- **Full Benchmark:** `README.md`\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "After tests pass:\n",
    "\n",
    "1. **Run CIFAR-100 Benchmark** (30-60 minutes)\n",
    "   ```bash\n",
    "   python scripts/run_benchmark.py --config configs/default.yaml\n",
    "   ```\n",
    "\n",
    "2. **Prepare ImageNet-1K** (if you have the data)\n",
    "   ```bash\n",
    "   python scripts/run_benchmark.py --config configs/imagenet_full.yaml\n",
    "   ```\n",
    "\n",
    "3. **Add Your Own Model**\n",
    "   - Implement `BaseSSLModel` in `models/`\n",
    "   - Register in `models/__init__.py`\n",
    "   - Run sanity check to validate\n",
    "   - Run benchmark to compare\n",
    "\n",
    "## ‚ú® Key Features\n",
    "\n",
    "‚úÖ **No dataset downloads** - All tests use synthetic data\n",
    "‚úÖ **Works on any GPU** - Auto-detects CUDA, MPS, or CPU\n",
    "‚úÖ **Fast feedback** - Unit tests in 30-60 seconds\n",
    "‚úÖ **Comprehensive** - 50+ tests covering all components\n",
    "‚úÖ **Production ready** - Proper fixtures, markers, parametrization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "footer"
   },
   "source": [
    "---\n",
    "\n",
    "**Created:** February 2025\n",
    "\n",
    "**Status:** ‚úÖ Testing environment ready\n",
    "\n",
    "For issues or questions, check the documentation files or run individual test cells above."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
